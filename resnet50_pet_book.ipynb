{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Project Title: Fine-Tuning ResNet50 on Oxford-IIIT Pet Dataset**  \n",
    "**Description:**  \n",
    "This project demonstrates the fine-tuning of a pre-trained ResNet50 model on the Oxford-IIIT Pet dataset. The dataset contains 37 classes of cats and dogs, and the goal is to classify images accurately into these categories. Using transfer learning, the ResNet50 model, pre-trained on ImageNet, was adapted to this specific task by modifying the final layers and optimizing the model for the pet classification problem. The notebook covers data preprocessing, model training, evaluation, and predictions on sample images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downloading module from GitHub by mrdbourke\n",
    "use module in going_modular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Checking if required files and folders already exist.\n",
      "[INFO] Downloading module from GitHub by mrdbourke.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'pytorch-deep-learning'...\n",
      "Updating files:  54% (135/248)\n",
      "Updating files:  55% (137/248)\n",
      "Updating files:  56% (139/248)\n",
      "Updating files:  57% (142/248)\n",
      "Updating files:  58% (144/248)\n",
      "Updating files:  59% (147/248)\n",
      "Updating files:  60% (149/248)\n",
      "Updating files:  61% (152/248)\n",
      "Updating files:  62% (154/248)\n",
      "Updating files:  63% (157/248)\n",
      "Updating files:  64% (159/248)\n",
      "Updating files:  65% (162/248)\n",
      "Updating files:  66% (164/248)\n",
      "Updating files:  67% (167/248)\n",
      "Updating files:  68% (169/248)\n",
      "Updating files:  69% (172/248)\n",
      "Updating files:  70% (174/248)\n",
      "Updating files:  71% (177/248)\n",
      "Updating files:  72% (179/248)\n",
      "Updating files:  73% (182/248)\n",
      "Updating files:  74% (184/248)\n",
      "Updating files:  75% (186/248)\n",
      "Updating files:  76% (189/248)\n",
      "Updating files:  77% (191/248)\n",
      "Updating files:  78% (194/248)\n",
      "Updating files:  79% (196/248)\n",
      "Updating files:  80% (199/248)\n",
      "Updating files:  81% (201/248)\n",
      "Updating files:  82% (204/248)\n",
      "Updating files:  83% (206/248)\n",
      "Updating files:  84% (209/248)\n",
      "Updating files:  85% (211/248)\n",
      "Updating files:  86% (214/248)\n",
      "Updating files:  87% (216/248)\n",
      "Updating files:  88% (219/248)\n",
      "Updating files:  89% (221/248)\n",
      "Updating files:  90% (224/248)\n",
      "Updating files:  91% (226/248)\n",
      "Updating files:  92% (229/248)\n",
      "Updating files:  93% (231/248)\n",
      "Updating files:  94% (234/248)\n",
      "Updating files:  95% (236/248)\n",
      "Updating files:  96% (239/248)\n",
      "Updating files:  97% (241/248)\n",
      "Updating files:  98% (244/248)\n",
      "Updating files:  99% (246/248)\n",
      "Updating files: 100% (248/248)\n",
      "Updating files: 100% (248/248), done.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        1 dir(s) moved.\n",
      "        1 file(s) moved.\n",
      "[INFO] Download and setup completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The system cannot find the file specified.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "print(\"[INFO] Checking if required files and folders already exist.\")\n",
    "\n",
    "# ตรวจสอบว่ามีโฟลเดอร์ 'going_modular' และไฟล์ 'helper_functions.py' แล้วหรือไม่\n",
    "if os.path.exists(\"going_modular\") and os.path.exists(\"going_modular/helper_functions.py\"):\n",
    "    print(\"[INFO] Required files and folders already exist. Skipping download.\")\n",
    "else:\n",
    "    print(\"[INFO] Downloading module from GitHub by mrdbourke.\")\n",
    "    # Clone repository\n",
    "    !git clone https://github.com/mrdbourke/pytorch-deep-learning\n",
    "    \n",
    "    # Move 'going_modular' to the current directory\n",
    "    !move pytorch-deep-learning\\going_modular .\n",
    "    \n",
    "    # Move 'helper_functions.py' to the 'going_modular' folder\n",
    "    !move pytorch-deep-learning\\helper_functions.py going_modular\n",
    "    \n",
    "    # Remove the cloned repository\n",
    "    !rmdir /s /q pytorch-deep-learning\n",
    "    \n",
    "    # Remove 'going_modular/models' folder if it exists\n",
    "    !rmdir /s /q going_modular\\models\n",
    "    !rmdir /s /q pytorch-deep-learning\n",
    "    print(\"[INFO] Download and setup completed.\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\Deep_learning\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch \n",
    "import torchvision \n",
    "import matplotlib.pyplot as plt \n",
    "import pathlib\n",
    "import torchvision.transforms as transforms\n",
    "import pandas as pd\n",
    "from going_modular.going_modular import data_setup,engine,utils\n",
    "from going_modular.helper_functions import download_data, set_seeds, plot_loss_curves\n",
    "from torch import nn\n",
    "from torchinfo import summary\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from timeit import default_timer as timer\n",
    "from tqdm.auto import tqdm\n",
    "from typing import List, Dict ,Tuple\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a model for resnet50 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_resnet50_model(num_classes:int=3,\n",
    "                          seed:int=42,\n",
    "                          dropout_rate: float = 0.3):  # เพิ่ม dropout_rate\n",
    "  # 1. Setup pretrained ResNet50 weights\n",
    "  weights = torchvision.models.ResNet50_Weights.DEFAULT  # Use the default weights\n",
    "  \n",
    "  # 2. Get ResNet50 transforms\n",
    "  transforms = weights.transforms()\n",
    "  \n",
    "  # 3. Setup pretrained model instance\n",
    "  model = torchvision.models.resnet50(weights=weights)  # Use the pretrained ResNet50\n",
    "  \n",
    "  # 4. Freeze the base layers in the model (this will stop all layers from training)\n",
    "  for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "  \n",
    "  # 5. Change classifier head with random seed for reproducibility\n",
    "  torch.manual_seed(seed)\n",
    "  \n",
    "  # เพิ่ม Dropout และกำหนด output เป็น num_classes\n",
    "  model.fc = nn.Sequential(\n",
    "    nn.Dropout(p=dropout_rate),  # เพิ่ม dropout\n",
    "    nn.Linear(in_features=model.fc.in_features, out_features=num_classes, bias=True)\n",
    "  )\n",
    "  \n",
    "  return model, transforms\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "call function create_resnet50_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "resnet50_pets, resnet50_transforms = create_resnet50_model(num_classes=37,dropout_rate=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ImageClassification(\n",
       "    crop_size=[224]\n",
       "    resize_size=[232]\n",
       "    mean=[0.485, 0.456, 0.406]\n",
       "    std=[0.229, 0.224, 0.225]\n",
       "    interpolation=InterpolationMode.BILINEAR\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resnet50_transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Print ResNet50 model summary (uncomment for full output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "========================================================================================================================\n",
       "Layer (type (var_name))                  Input Shape          Output Shape         Param #              Trainable\n",
       "========================================================================================================================\n",
       "ResNet (ResNet)                          [1, 3, 224, 224]     [1, 37]              --                   Partial\n",
       "├─Conv2d (conv1)                         [1, 3, 224, 224]     [1, 64, 112, 112]    (9,408)              False\n",
       "├─BatchNorm2d (bn1)                      [1, 64, 112, 112]    [1, 64, 112, 112]    (128)                False\n",
       "├─ReLU (relu)                            [1, 64, 112, 112]    [1, 64, 112, 112]    --                   --\n",
       "├─MaxPool2d (maxpool)                    [1, 64, 112, 112]    [1, 64, 56, 56]      --                   --\n",
       "├─Sequential (layer1)                    [1, 64, 56, 56]      [1, 256, 56, 56]     --                   False\n",
       "│    └─Bottleneck (0)                    [1, 64, 56, 56]      [1, 256, 56, 56]     --                   False\n",
       "│    │    └─Conv2d (conv1)               [1, 64, 56, 56]      [1, 64, 56, 56]      (4,096)              False\n",
       "│    │    └─BatchNorm2d (bn1)            [1, 64, 56, 56]      [1, 64, 56, 56]      (128)                False\n",
       "│    │    └─ReLU (relu)                  [1, 64, 56, 56]      [1, 64, 56, 56]      --                   --\n",
       "│    │    └─Conv2d (conv2)               [1, 64, 56, 56]      [1, 64, 56, 56]      (36,864)             False\n",
       "│    │    └─BatchNorm2d (bn2)            [1, 64, 56, 56]      [1, 64, 56, 56]      (128)                False\n",
       "│    │    └─ReLU (relu)                  [1, 64, 56, 56]      [1, 64, 56, 56]      --                   --\n",
       "│    │    └─Conv2d (conv3)               [1, 64, 56, 56]      [1, 256, 56, 56]     (16,384)             False\n",
       "│    │    └─BatchNorm2d (bn3)            [1, 256, 56, 56]     [1, 256, 56, 56]     (512)                False\n",
       "│    │    └─Sequential (downsample)      [1, 64, 56, 56]      [1, 256, 56, 56]     (16,896)             False\n",
       "│    │    └─ReLU (relu)                  [1, 256, 56, 56]     [1, 256, 56, 56]     --                   --\n",
       "│    └─Bottleneck (1)                    [1, 256, 56, 56]     [1, 256, 56, 56]     --                   False\n",
       "│    │    └─Conv2d (conv1)               [1, 256, 56, 56]     [1, 64, 56, 56]      (16,384)             False\n",
       "│    │    └─BatchNorm2d (bn1)            [1, 64, 56, 56]      [1, 64, 56, 56]      (128)                False\n",
       "│    │    └─ReLU (relu)                  [1, 64, 56, 56]      [1, 64, 56, 56]      --                   --\n",
       "│    │    └─Conv2d (conv2)               [1, 64, 56, 56]      [1, 64, 56, 56]      (36,864)             False\n",
       "│    │    └─BatchNorm2d (bn2)            [1, 64, 56, 56]      [1, 64, 56, 56]      (128)                False\n",
       "│    │    └─ReLU (relu)                  [1, 64, 56, 56]      [1, 64, 56, 56]      --                   --\n",
       "│    │    └─Conv2d (conv3)               [1, 64, 56, 56]      [1, 256, 56, 56]     (16,384)             False\n",
       "│    │    └─BatchNorm2d (bn3)            [1, 256, 56, 56]     [1, 256, 56, 56]     (512)                False\n",
       "│    │    └─ReLU (relu)                  [1, 256, 56, 56]     [1, 256, 56, 56]     --                   --\n",
       "│    └─Bottleneck (2)                    [1, 256, 56, 56]     [1, 256, 56, 56]     --                   False\n",
       "│    │    └─Conv2d (conv1)               [1, 256, 56, 56]     [1, 64, 56, 56]      (16,384)             False\n",
       "│    │    └─BatchNorm2d (bn1)            [1, 64, 56, 56]      [1, 64, 56, 56]      (128)                False\n",
       "│    │    └─ReLU (relu)                  [1, 64, 56, 56]      [1, 64, 56, 56]      --                   --\n",
       "│    │    └─Conv2d (conv2)               [1, 64, 56, 56]      [1, 64, 56, 56]      (36,864)             False\n",
       "│    │    └─BatchNorm2d (bn2)            [1, 64, 56, 56]      [1, 64, 56, 56]      (128)                False\n",
       "│    │    └─ReLU (relu)                  [1, 64, 56, 56]      [1, 64, 56, 56]      --                   --\n",
       "│    │    └─Conv2d (conv3)               [1, 64, 56, 56]      [1, 256, 56, 56]     (16,384)             False\n",
       "│    │    └─BatchNorm2d (bn3)            [1, 256, 56, 56]     [1, 256, 56, 56]     (512)                False\n",
       "│    │    └─ReLU (relu)                  [1, 256, 56, 56]     [1, 256, 56, 56]     --                   --\n",
       "├─Sequential (layer2)                    [1, 256, 56, 56]     [1, 512, 28, 28]     --                   False\n",
       "│    └─Bottleneck (0)                    [1, 256, 56, 56]     [1, 512, 28, 28]     --                   False\n",
       "│    │    └─Conv2d (conv1)               [1, 256, 56, 56]     [1, 128, 56, 56]     (32,768)             False\n",
       "│    │    └─BatchNorm2d (bn1)            [1, 128, 56, 56]     [1, 128, 56, 56]     (256)                False\n",
       "│    │    └─ReLU (relu)                  [1, 128, 56, 56]     [1, 128, 56, 56]     --                   --\n",
       "│    │    └─Conv2d (conv2)               [1, 128, 56, 56]     [1, 128, 28, 28]     (147,456)            False\n",
       "│    │    └─BatchNorm2d (bn2)            [1, 128, 28, 28]     [1, 128, 28, 28]     (256)                False\n",
       "│    │    └─ReLU (relu)                  [1, 128, 28, 28]     [1, 128, 28, 28]     --                   --\n",
       "│    │    └─Conv2d (conv3)               [1, 128, 28, 28]     [1, 512, 28, 28]     (65,536)             False\n",
       "│    │    └─BatchNorm2d (bn3)            [1, 512, 28, 28]     [1, 512, 28, 28]     (1,024)              False\n",
       "│    │    └─Sequential (downsample)      [1, 256, 56, 56]     [1, 512, 28, 28]     (132,096)            False\n",
       "│    │    └─ReLU (relu)                  [1, 512, 28, 28]     [1, 512, 28, 28]     --                   --\n",
       "│    └─Bottleneck (1)                    [1, 512, 28, 28]     [1, 512, 28, 28]     --                   False\n",
       "│    │    └─Conv2d (conv1)               [1, 512, 28, 28]     [1, 128, 28, 28]     (65,536)             False\n",
       "│    │    └─BatchNorm2d (bn1)            [1, 128, 28, 28]     [1, 128, 28, 28]     (256)                False\n",
       "│    │    └─ReLU (relu)                  [1, 128, 28, 28]     [1, 128, 28, 28]     --                   --\n",
       "│    │    └─Conv2d (conv2)               [1, 128, 28, 28]     [1, 128, 28, 28]     (147,456)            False\n",
       "│    │    └─BatchNorm2d (bn2)            [1, 128, 28, 28]     [1, 128, 28, 28]     (256)                False\n",
       "│    │    └─ReLU (relu)                  [1, 128, 28, 28]     [1, 128, 28, 28]     --                   --\n",
       "│    │    └─Conv2d (conv3)               [1, 128, 28, 28]     [1, 512, 28, 28]     (65,536)             False\n",
       "│    │    └─BatchNorm2d (bn3)            [1, 512, 28, 28]     [1, 512, 28, 28]     (1,024)              False\n",
       "│    │    └─ReLU (relu)                  [1, 512, 28, 28]     [1, 512, 28, 28]     --                   --\n",
       "│    └─Bottleneck (2)                    [1, 512, 28, 28]     [1, 512, 28, 28]     --                   False\n",
       "│    │    └─Conv2d (conv1)               [1, 512, 28, 28]     [1, 128, 28, 28]     (65,536)             False\n",
       "│    │    └─BatchNorm2d (bn1)            [1, 128, 28, 28]     [1, 128, 28, 28]     (256)                False\n",
       "│    │    └─ReLU (relu)                  [1, 128, 28, 28]     [1, 128, 28, 28]     --                   --\n",
       "│    │    └─Conv2d (conv2)               [1, 128, 28, 28]     [1, 128, 28, 28]     (147,456)            False\n",
       "│    │    └─BatchNorm2d (bn2)            [1, 128, 28, 28]     [1, 128, 28, 28]     (256)                False\n",
       "│    │    └─ReLU (relu)                  [1, 128, 28, 28]     [1, 128, 28, 28]     --                   --\n",
       "│    │    └─Conv2d (conv3)               [1, 128, 28, 28]     [1, 512, 28, 28]     (65,536)             False\n",
       "│    │    └─BatchNorm2d (bn3)            [1, 512, 28, 28]     [1, 512, 28, 28]     (1,024)              False\n",
       "│    │    └─ReLU (relu)                  [1, 512, 28, 28]     [1, 512, 28, 28]     --                   --\n",
       "│    └─Bottleneck (3)                    [1, 512, 28, 28]     [1, 512, 28, 28]     --                   False\n",
       "│    │    └─Conv2d (conv1)               [1, 512, 28, 28]     [1, 128, 28, 28]     (65,536)             False\n",
       "│    │    └─BatchNorm2d (bn1)            [1, 128, 28, 28]     [1, 128, 28, 28]     (256)                False\n",
       "│    │    └─ReLU (relu)                  [1, 128, 28, 28]     [1, 128, 28, 28]     --                   --\n",
       "│    │    └─Conv2d (conv2)               [1, 128, 28, 28]     [1, 128, 28, 28]     (147,456)            False\n",
       "│    │    └─BatchNorm2d (bn2)            [1, 128, 28, 28]     [1, 128, 28, 28]     (256)                False\n",
       "│    │    └─ReLU (relu)                  [1, 128, 28, 28]     [1, 128, 28, 28]     --                   --\n",
       "│    │    └─Conv2d (conv3)               [1, 128, 28, 28]     [1, 512, 28, 28]     (65,536)             False\n",
       "│    │    └─BatchNorm2d (bn3)            [1, 512, 28, 28]     [1, 512, 28, 28]     (1,024)              False\n",
       "│    │    └─ReLU (relu)                  [1, 512, 28, 28]     [1, 512, 28, 28]     --                   --\n",
       "├─Sequential (layer3)                    [1, 512, 28, 28]     [1, 1024, 14, 14]    --                   False\n",
       "│    └─Bottleneck (0)                    [1, 512, 28, 28]     [1, 1024, 14, 14]    --                   False\n",
       "│    │    └─Conv2d (conv1)               [1, 512, 28, 28]     [1, 256, 28, 28]     (131,072)            False\n",
       "│    │    └─BatchNorm2d (bn1)            [1, 256, 28, 28]     [1, 256, 28, 28]     (512)                False\n",
       "│    │    └─ReLU (relu)                  [1, 256, 28, 28]     [1, 256, 28, 28]     --                   --\n",
       "│    │    └─Conv2d (conv2)               [1, 256, 28, 28]     [1, 256, 14, 14]     (589,824)            False\n",
       "│    │    └─BatchNorm2d (bn2)            [1, 256, 14, 14]     [1, 256, 14, 14]     (512)                False\n",
       "│    │    └─ReLU (relu)                  [1, 256, 14, 14]     [1, 256, 14, 14]     --                   --\n",
       "│    │    └─Conv2d (conv3)               [1, 256, 14, 14]     [1, 1024, 14, 14]    (262,144)            False\n",
       "│    │    └─BatchNorm2d (bn3)            [1, 1024, 14, 14]    [1, 1024, 14, 14]    (2,048)              False\n",
       "│    │    └─Sequential (downsample)      [1, 512, 28, 28]     [1, 1024, 14, 14]    (526,336)            False\n",
       "│    │    └─ReLU (relu)                  [1, 1024, 14, 14]    [1, 1024, 14, 14]    --                   --\n",
       "│    └─Bottleneck (1)                    [1, 1024, 14, 14]    [1, 1024, 14, 14]    --                   False\n",
       "│    │    └─Conv2d (conv1)               [1, 1024, 14, 14]    [1, 256, 14, 14]     (262,144)            False\n",
       "│    │    └─BatchNorm2d (bn1)            [1, 256, 14, 14]     [1, 256, 14, 14]     (512)                False\n",
       "│    │    └─ReLU (relu)                  [1, 256, 14, 14]     [1, 256, 14, 14]     --                   --\n",
       "│    │    └─Conv2d (conv2)               [1, 256, 14, 14]     [1, 256, 14, 14]     (589,824)            False\n",
       "│    │    └─BatchNorm2d (bn2)            [1, 256, 14, 14]     [1, 256, 14, 14]     (512)                False\n",
       "│    │    └─ReLU (relu)                  [1, 256, 14, 14]     [1, 256, 14, 14]     --                   --\n",
       "│    │    └─Conv2d (conv3)               [1, 256, 14, 14]     [1, 1024, 14, 14]    (262,144)            False\n",
       "│    │    └─BatchNorm2d (bn3)            [1, 1024, 14, 14]    [1, 1024, 14, 14]    (2,048)              False\n",
       "│    │    └─ReLU (relu)                  [1, 1024, 14, 14]    [1, 1024, 14, 14]    --                   --\n",
       "│    └─Bottleneck (2)                    [1, 1024, 14, 14]    [1, 1024, 14, 14]    --                   False\n",
       "│    │    └─Conv2d (conv1)               [1, 1024, 14, 14]    [1, 256, 14, 14]     (262,144)            False\n",
       "│    │    └─BatchNorm2d (bn1)            [1, 256, 14, 14]     [1, 256, 14, 14]     (512)                False\n",
       "│    │    └─ReLU (relu)                  [1, 256, 14, 14]     [1, 256, 14, 14]     --                   --\n",
       "│    │    └─Conv2d (conv2)               [1, 256, 14, 14]     [1, 256, 14, 14]     (589,824)            False\n",
       "│    │    └─BatchNorm2d (bn2)            [1, 256, 14, 14]     [1, 256, 14, 14]     (512)                False\n",
       "│    │    └─ReLU (relu)                  [1, 256, 14, 14]     [1, 256, 14, 14]     --                   --\n",
       "│    │    └─Conv2d (conv3)               [1, 256, 14, 14]     [1, 1024, 14, 14]    (262,144)            False\n",
       "│    │    └─BatchNorm2d (bn3)            [1, 1024, 14, 14]    [1, 1024, 14, 14]    (2,048)              False\n",
       "│    │    └─ReLU (relu)                  [1, 1024, 14, 14]    [1, 1024, 14, 14]    --                   --\n",
       "│    └─Bottleneck (3)                    [1, 1024, 14, 14]    [1, 1024, 14, 14]    --                   False\n",
       "│    │    └─Conv2d (conv1)               [1, 1024, 14, 14]    [1, 256, 14, 14]     (262,144)            False\n",
       "│    │    └─BatchNorm2d (bn1)            [1, 256, 14, 14]     [1, 256, 14, 14]     (512)                False\n",
       "│    │    └─ReLU (relu)                  [1, 256, 14, 14]     [1, 256, 14, 14]     --                   --\n",
       "│    │    └─Conv2d (conv2)               [1, 256, 14, 14]     [1, 256, 14, 14]     (589,824)            False\n",
       "│    │    └─BatchNorm2d (bn2)            [1, 256, 14, 14]     [1, 256, 14, 14]     (512)                False\n",
       "│    │    └─ReLU (relu)                  [1, 256, 14, 14]     [1, 256, 14, 14]     --                   --\n",
       "│    │    └─Conv2d (conv3)               [1, 256, 14, 14]     [1, 1024, 14, 14]    (262,144)            False\n",
       "│    │    └─BatchNorm2d (bn3)            [1, 1024, 14, 14]    [1, 1024, 14, 14]    (2,048)              False\n",
       "│    │    └─ReLU (relu)                  [1, 1024, 14, 14]    [1, 1024, 14, 14]    --                   --\n",
       "│    └─Bottleneck (4)                    [1, 1024, 14, 14]    [1, 1024, 14, 14]    --                   False\n",
       "│    │    └─Conv2d (conv1)               [1, 1024, 14, 14]    [1, 256, 14, 14]     (262,144)            False\n",
       "│    │    └─BatchNorm2d (bn1)            [1, 256, 14, 14]     [1, 256, 14, 14]     (512)                False\n",
       "│    │    └─ReLU (relu)                  [1, 256, 14, 14]     [1, 256, 14, 14]     --                   --\n",
       "│    │    └─Conv2d (conv2)               [1, 256, 14, 14]     [1, 256, 14, 14]     (589,824)            False\n",
       "│    │    └─BatchNorm2d (bn2)            [1, 256, 14, 14]     [1, 256, 14, 14]     (512)                False\n",
       "│    │    └─ReLU (relu)                  [1, 256, 14, 14]     [1, 256, 14, 14]     --                   --\n",
       "│    │    └─Conv2d (conv3)               [1, 256, 14, 14]     [1, 1024, 14, 14]    (262,144)            False\n",
       "│    │    └─BatchNorm2d (bn3)            [1, 1024, 14, 14]    [1, 1024, 14, 14]    (2,048)              False\n",
       "│    │    └─ReLU (relu)                  [1, 1024, 14, 14]    [1, 1024, 14, 14]    --                   --\n",
       "│    └─Bottleneck (5)                    [1, 1024, 14, 14]    [1, 1024, 14, 14]    --                   False\n",
       "│    │    └─Conv2d (conv1)               [1, 1024, 14, 14]    [1, 256, 14, 14]     (262,144)            False\n",
       "│    │    └─BatchNorm2d (bn1)            [1, 256, 14, 14]     [1, 256, 14, 14]     (512)                False\n",
       "│    │    └─ReLU (relu)                  [1, 256, 14, 14]     [1, 256, 14, 14]     --                   --\n",
       "│    │    └─Conv2d (conv2)               [1, 256, 14, 14]     [1, 256, 14, 14]     (589,824)            False\n",
       "│    │    └─BatchNorm2d (bn2)            [1, 256, 14, 14]     [1, 256, 14, 14]     (512)                False\n",
       "│    │    └─ReLU (relu)                  [1, 256, 14, 14]     [1, 256, 14, 14]     --                   --\n",
       "│    │    └─Conv2d (conv3)               [1, 256, 14, 14]     [1, 1024, 14, 14]    (262,144)            False\n",
       "│    │    └─BatchNorm2d (bn3)            [1, 1024, 14, 14]    [1, 1024, 14, 14]    (2,048)              False\n",
       "│    │    └─ReLU (relu)                  [1, 1024, 14, 14]    [1, 1024, 14, 14]    --                   --\n",
       "├─Sequential (layer4)                    [1, 1024, 14, 14]    [1, 2048, 7, 7]      --                   False\n",
       "│    └─Bottleneck (0)                    [1, 1024, 14, 14]    [1, 2048, 7, 7]      --                   False\n",
       "│    │    └─Conv2d (conv1)               [1, 1024, 14, 14]    [1, 512, 14, 14]     (524,288)            False\n",
       "│    │    └─BatchNorm2d (bn1)            [1, 512, 14, 14]     [1, 512, 14, 14]     (1,024)              False\n",
       "│    │    └─ReLU (relu)                  [1, 512, 14, 14]     [1, 512, 14, 14]     --                   --\n",
       "│    │    └─Conv2d (conv2)               [1, 512, 14, 14]     [1, 512, 7, 7]       (2,359,296)          False\n",
       "│    │    └─BatchNorm2d (bn2)            [1, 512, 7, 7]       [1, 512, 7, 7]       (1,024)              False\n",
       "│    │    └─ReLU (relu)                  [1, 512, 7, 7]       [1, 512, 7, 7]       --                   --\n",
       "│    │    └─Conv2d (conv3)               [1, 512, 7, 7]       [1, 2048, 7, 7]      (1,048,576)          False\n",
       "│    │    └─BatchNorm2d (bn3)            [1, 2048, 7, 7]      [1, 2048, 7, 7]      (4,096)              False\n",
       "│    │    └─Sequential (downsample)      [1, 1024, 14, 14]    [1, 2048, 7, 7]      (2,101,248)          False\n",
       "│    │    └─ReLU (relu)                  [1, 2048, 7, 7]      [1, 2048, 7, 7]      --                   --\n",
       "│    └─Bottleneck (1)                    [1, 2048, 7, 7]      [1, 2048, 7, 7]      --                   False\n",
       "│    │    └─Conv2d (conv1)               [1, 2048, 7, 7]      [1, 512, 7, 7]       (1,048,576)          False\n",
       "│    │    └─BatchNorm2d (bn1)            [1, 512, 7, 7]       [1, 512, 7, 7]       (1,024)              False\n",
       "│    │    └─ReLU (relu)                  [1, 512, 7, 7]       [1, 512, 7, 7]       --                   --\n",
       "│    │    └─Conv2d (conv2)               [1, 512, 7, 7]       [1, 512, 7, 7]       (2,359,296)          False\n",
       "│    │    └─BatchNorm2d (bn2)            [1, 512, 7, 7]       [1, 512, 7, 7]       (1,024)              False\n",
       "│    │    └─ReLU (relu)                  [1, 512, 7, 7]       [1, 512, 7, 7]       --                   --\n",
       "│    │    └─Conv2d (conv3)               [1, 512, 7, 7]       [1, 2048, 7, 7]      (1,048,576)          False\n",
       "│    │    └─BatchNorm2d (bn3)            [1, 2048, 7, 7]      [1, 2048, 7, 7]      (4,096)              False\n",
       "│    │    └─ReLU (relu)                  [1, 2048, 7, 7]      [1, 2048, 7, 7]      --                   --\n",
       "│    └─Bottleneck (2)                    [1, 2048, 7, 7]      [1, 2048, 7, 7]      --                   False\n",
       "│    │    └─Conv2d (conv1)               [1, 2048, 7, 7]      [1, 512, 7, 7]       (1,048,576)          False\n",
       "│    │    └─BatchNorm2d (bn1)            [1, 512, 7, 7]       [1, 512, 7, 7]       (1,024)              False\n",
       "│    │    └─ReLU (relu)                  [1, 512, 7, 7]       [1, 512, 7, 7]       --                   --\n",
       "│    │    └─Conv2d (conv2)               [1, 512, 7, 7]       [1, 512, 7, 7]       (2,359,296)          False\n",
       "│    │    └─BatchNorm2d (bn2)            [1, 512, 7, 7]       [1, 512, 7, 7]       (1,024)              False\n",
       "│    │    └─ReLU (relu)                  [1, 512, 7, 7]       [1, 512, 7, 7]       --                   --\n",
       "│    │    └─Conv2d (conv3)               [1, 512, 7, 7]       [1, 2048, 7, 7]      (1,048,576)          False\n",
       "│    │    └─BatchNorm2d (bn3)            [1, 2048, 7, 7]      [1, 2048, 7, 7]      (4,096)              False\n",
       "│    │    └─ReLU (relu)                  [1, 2048, 7, 7]      [1, 2048, 7, 7]      --                   --\n",
       "├─AdaptiveAvgPool2d (avgpool)            [1, 2048, 7, 7]      [1, 2048, 1, 1]      --                   --\n",
       "├─Sequential (fc)                        [1, 2048]            [1, 37]              --                   True\n",
       "│    └─Dropout (0)                       [1, 2048]            [1, 2048]            --                   --\n",
       "│    └─Linear (1)                        [1, 2048]            [1, 37]              75,813               True\n",
       "========================================================================================================================\n",
       "Total params: 23,583,845\n",
       "Trainable params: 75,813\n",
       "Non-trainable params: 23,508,032\n",
       "Total mult-adds (Units.GIGABYTES): 4.09\n",
       "========================================================================================================================\n",
       "Input size (MB): 0.60\n",
       "Forward/backward pass size (MB): 177.82\n",
       "Params size (MB): 94.34\n",
       "Estimated Total Size (MB): 272.76\n",
       "========================================================================================================================"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(resnet50_pets,\n",
    "        input_size=(1, 3, 224, 224),\n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "        col_width=20,\n",
    "        row_settings=[\"var_names\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create DataLoaders for resnet50 feature extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(WindowsPath('img/train'), WindowsPath('img/test'))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_pet_path = Path(\"./img\")\n",
    "train_dir = data_pet_path / \"train\"\n",
    "test_dir = data_pet_path / \"test\"\n",
    "train_dir,test_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check cpu core\n",
    "Count cpu for use on num_workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os_cpu_count = os.cpu_count()\n",
    "os_cpu_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup resnet50 dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup resnet50 dataloaders\n",
    "from going_modular.going_modular import data_setup\n",
    "train_dataloader_resnet50_pet_pretrained, test_dataloader_resnet50_pet_pretrained, class_names = data_setup.create_dataloaders(train_dir=train_dir,\n",
    "                                                                                                     test_dir=test_dir,\n",
    "                                                                                                     transform=resnet50_transforms,\n",
    "                                                                                                     batch_size=32,\n",
    "                                                                                                     num_workers=16) # Could increase if we had more samples, such as here: https://arxiv.org/abs/2205.01580 (there are other improvements there too...)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "checking batch size each dataloder from \n",
    "train_dataloader_pet_pretrained and test_dataloader_pet_pretrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(149,\n",
       " 47,\n",
       " ['Abyssinian',\n",
       "  'Bengal',\n",
       "  'Birman',\n",
       "  'Bombay',\n",
       "  'British_Shorthair',\n",
       "  'Egyptian_Mau',\n",
       "  'Maine_Coon',\n",
       "  'Persian',\n",
       "  'Ragdoll',\n",
       "  'Russian_Blue'])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataloader_resnet50_pet_pretrained),len(test_dataloader_resnet50_pet_pretrained),class_names[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training resnet50 Feature \n",
    "**`lr=1e-3 = 0.001`** is a suitable starting value, especially for medium to large models (e.g., ResNet or EfficientNet) and optimizers like Adam or AdamW."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1/5 [02:59<11:59, 179.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | train_loss: 1.7972 | train_acc: 0.6758 | test_loss: 0.8676 | test_acc: 0.8863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 2/5 [06:03<09:06, 182.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 | train_loss: 0.5926 | train_acc: 0.9121 | test_loss: 0.5158 | test_acc: 0.9076\n"
     ]
    }
   ],
   "source": [
    "# Create optimizer and loss function\n",
    "optimizer = torch.optim.Adam(params=resnet50_pets.parameters(),\n",
    "                             lr=1e-3)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Train the classifier head of the pretrained resnet50 feature extractor model\n",
    "set_seeds()\n",
    "pretrained_resnet50_pets_results = engine.train(model=resnet50_pets,\n",
    "                                      train_dataloader=train_dataloader_resnet50_pet_pretrained,\n",
    "                                      test_dataloader=test_dataloader_resnet50_pet_pretrained,\n",
    "                                      optimizer=optimizer,\n",
    "                                      loss_fn=loss_fn,\n",
    "                                      epochs=5,\n",
    "                                      device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plot loss curves "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_curves(pretrained_resnet50_pets_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving resnet50 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.save_model(model=resnet50_pets,\n",
    "                 target_dir=\"models\",\n",
    "                 model_name=\"pretrained_resnet50_pets.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the model size in bytes and convert to megabytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_resnet50_model_size = Path(\"models/pretrained_resnet50_pets.pth\").stat().st_size / (1024 * 1024)\n",
    "print(f\"Pretrained Resnet50 feature extractor model size: {round(pretrained_resnet50_model_size, 2)} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count number of parameters in model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet50_pets_total_params = sum(torch.numel(param) for param in resnet50_pets.parameters())\n",
    "resnet50_pets_total_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Create a dictionary with Resnet50 Pet for keep records statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet50_pets_status = {\"test_loss\": pretrained_resnet50_pets_results[\"test_loss\"][-1],\n",
    "                  \"test_acc\": pretrained_resnet50_pets_results[\"test_acc\"][-1],\n",
    "                  \"number_of_parameters\": resnet50_pets_total_params,\n",
    "                  \"model_size (MB)\": pretrained_resnet50_model_size}\n",
    "\n",
    "resnet50_pets_status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize Visualize Visualize data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making predictions with our trained models and timing them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Making predictions with our trained models and timing them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all test data img \n",
    "test_data_paths = list(Path(test_dir).glob(\"*/*.jpg\"))\n",
    "test_data_paths[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a function to make across the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict resnet50_pets\n",
    "from going_modular.going_modular import predictions\n",
    "resnet50_pets_test_pred_dicts  = pred_and_store(model = resnet50_pets,\n",
    "                         target_paths = test_data_paths,\n",
    "                         transforms = resnet50_transforms,\n",
    "                         class_names = class_names,\n",
    "                         device = \"cpu\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turn the resnet50_pets_test_pred_dicts into DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet50_pets_test_pred_df = pd.DataFrame(resnet50_pets_test_pred_dicts)\n",
    "resnet50_pets_test_pred_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check number of correct predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet50_pets_test_pred_df.correct.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the avg time per prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet50_pets_average_time_per_pred  = round(resnet50_pets_test_pred_df.prediction_time.mean(),4)\n",
    "print(f\"avg per sec. predition : {resnet50_pets_average_time_per_pred}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add time per pred to Resnet50 Pet stats dictionary record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet50_pets_status[\"time_per_pred_cpu\"] = resnet50_pets_average_time_per_pred\n",
    "resnet50_pets_status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Function Plot Perdict random Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_and_plot_image(\n",
    "    model: torch.nn.Module,\n",
    "    class_names: List[str],\n",
    "    image_path: str,\n",
    "    image_size: Tuple[int, int] = (224, 224),\n",
    "    transform: torchvision.transforms = None,\n",
    "    device: torch.device = device,\n",
    "):\n",
    "    \"\"\"Predicts on a target image with a target model.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): A trained (or untrained) PyTorch model to predict on an image.\n",
    "        class_names (List[str]): A list of target classes to map predictions to.\n",
    "        image_path (str): Filepath to target image to predict on.\n",
    "        image_size (Tuple[int, int], optional): Size to transform target image to. Defaults to (224, 224).\n",
    "        transform (torchvision.transforms, optional): Transform to perform on image. Defaults to None which uses ImageNet normalization.\n",
    "        device (torch.device, optional): Target device to perform prediction on. Defaults to device.\n",
    "    \"\"\"\n",
    "\n",
    "    # Open image\n",
    "    img = Image.open(image_path)\n",
    "\n",
    "    # Create transformation for image (if one doesn't exist)\n",
    "    if transform is not None:\n",
    "        image_transform = transform\n",
    "    else:\n",
    "        image_transform = transforms.Compose(\n",
    "            [\n",
    "                transforms.Resize(image_size),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(\n",
    "                    mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    ### Predict on image ###\n",
    "\n",
    "    # Make sure the model is on the target device\n",
    "    model.to(device)\n",
    "\n",
    "    # Turn on model evaluation mode and inference mode\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        # Transform and add an extra dimension to image (model requires samples in [batch_size, color_channels, height, width])\n",
    "        transformed_image = image_transform(img).unsqueeze(dim=0)\n",
    "\n",
    "        # Make a prediction on image with an extra dimension and send it to the target device\n",
    "        target_image_pred = model(transformed_image.to(device))\n",
    "\n",
    "    # Convert logits -> prediction probabilities (using torch.softmax() for multi-class classification)\n",
    "    target_image_pred_probs = torch.softmax(target_image_pred, dim=1)\n",
    "\n",
    "    # Convert prediction probabilities -> prediction labels\n",
    "    target_image_pred_label = torch.argmax(target_image_pred_probs, dim=1)\n",
    "\n",
    "    # Plot image with predicted label and probability\n",
    "    plt.figure()\n",
    "    plt.imshow(img)\n",
    "    plt.title(\n",
    "        f\"Pred: {class_names[target_image_pred_label]} | Prob: {target_image_pred_probs.max():.3f}\"\n",
    "    )\n",
    "    plt.axis(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show random 5 image perdiction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a random list of image paths from test set\n",
    "import random\n",
    "num_images_to_plot = 5\n",
    "test_image_path_list = list(Path(test_dir).glob(\"*/*.jpg\")) # get list all image paths from test data \n",
    "test_image_path_sample = random.sample(population=test_image_path_list, # go through all of the test image paths\n",
    "                                       k=num_images_to_plot) # randomly select 'k' image paths to pred and plot\n",
    "\n",
    "# Make predictions on and plot the images\n",
    "for image_path in test_image_path_sample:\n",
    "    pred_and_plot_image(model=resnet50_pets, \n",
    "                        image_path=image_path,\n",
    "                        class_names=class_names,\n",
    "                        # transform=weights.transforms(), # optionally pass in a specified transform from our pretrained model weights\n",
    "                        image_size=(224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo \"*.ipynb filter=nbstripout\" > .gitattributes\n",
    "!copy .gitattributes demos\\"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Model Summary**  \n",
    "The model achieved a **test accuracy of 92.09%** with a **test loss of 0.3186** on a 37-class classification task. It has **23,583,845 parameters** and a model size of **90.27 MB**, making it efficient for deployment.  \n",
    "\n",
    "Out of **1,479 total predictions**, the model correctly predicted **1,360 images** and misclassified **119 images**, resulting in an **error rate of approximately 8.05%**.  \n",
    "\n",
    "Further improvements could include fine-tuning with a lower learning rate, additional data augmentation, or error analysis for targeted enhancements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turning our Resnet50 Pet model into a deployable app via Hugingface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Folder demos/Resnet50_pet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "#Create FoodVision big demo path\n",
    "Resnet50_pet_demo_path = Path(\"demos\")\n",
    "\n",
    "#Remove files than might exist and create a new directorty (ถ้ามีไฟล์เก่าให้ลบและสร้างใหม่)\n",
    "if Resnet50_pet_demo_path.exists():\n",
    "  shutil.rmtree(Resnet50_pet_demo_path)\n",
    "  Resnet50_pet_demo_path.mkdir(parents=True, exist_ok=True)\n",
    "else:\n",
    "  Resnet50_pet_demo_path.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create an examples directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Resnet50_pet_examples_path = Resnet50_pet_demo_path / \"examples\"\n",
    "Resnet50_pet_examples_path.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Move training Model to demos/Resnet50_pet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!copy models\\pretrained_resnet50_pets.pth demos\\"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create path to Resnet50 Pet class names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet50_pet_classname_path = Resnet50_pet_demo_path / \"class_names.txt\"\n",
    "resnet50_pet_classname_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get classnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet50_pet_classname = class_names\n",
    "resnet50_pet_classname[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Record classname to class_names.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write Restnet50 Pet class names to text file\n",
    "with open(resnet50_pet_classname_path, \"w\") as f:\n",
    "  print(f\"[INFO] Saving Pet class names to {resnet50_pet_classname_path}\")\n",
    "  f.write(\"\\n\".join(resnet50_pet_classname)) # new line per class name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open and Read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open resnet50 pet class names file and read each line into a list\n",
    "with open(resnet50_pet_classname_path, \"r\") as f:\n",
    "  resnet50_pet_classname_loaded = [pet.strip() for pet in f.readlines()]\n",
    "resnet50_pet_classname_loaded[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turning our Resnet50 Pet model into a Python script (model.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile demos/model.py\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "\n",
    "def create_resnet50_model(num_classes:int=3,\n",
    "                          seed:int=42,\n",
    "                          dropout_rate: float = 0.3):  # เพิ่ม dropout_rate\n",
    "  # 1. Setup pretrained ResNet50 weights\n",
    "  weights = torchvision.models.ResNet50_Weights.DEFAULT  # Use the default weights\n",
    "  \n",
    "  # 2. Get ResNet50 transforms\n",
    "  transforms = weights.transforms()\n",
    "  \n",
    "  # 3. Setup pretrained model instance\n",
    "  model = torchvision.models.resnet50(weights=weights)  # Use the pretrained ResNet50\n",
    "  \n",
    "  # 4. Freeze the base layers in the model (this will stop all layers from training)\n",
    "  for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "  \n",
    "  # 5. Change classifier head with random seed for reproducibility\n",
    "  torch.manual_seed(seed)\n",
    "  \n",
    "  # เพิ่ม Dropout และกำหนด output เป็น num_classes\n",
    "  model.fc = nn.Sequential(\n",
    "    nn.Dropout(p=dropout_rate),  # เพิ่ม dropout\n",
    "    nn.Linear(in_features=model.fc.in_features, out_features=num_classes, bias=True)\n",
    "  )\n",
    "  \n",
    "  return model, transforms\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turning our Resnet50 Pet Gradio app into a Python script (app.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile demos/app.py\n",
    "\n",
    "# 1. imports and class names setup\n",
    "import gradio as gr\n",
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "\n",
    "from model import create_resnet50_model\n",
    "from timeit import default_timer as timer\n",
    "from typing import Tuple, Dict\n",
    "\n",
    "# Setup class names\n",
    "with open(\"class_names.txt\", \"r\") as f:\n",
    "  class_names = [pet.strip() for pet in f.readlines()]\n",
    "\n",
    "# 2. model and transforms preparation\n",
    "resnet50_pets, resnet50_transforms = create_resnet50_model(num_classes=37)\n",
    "\n",
    "# Load saved weights\n",
    "resnet50_pets.load_state_dict(\n",
    "    torch.load(\n",
    "        f=\"pretrained_resnet50_pets.pth\",\n",
    "        map_location=torch.device(\"cpu\") # load the model to the CPU\n",
    "    )\n",
    ")\n",
    "\n",
    "from typing import Tuple, Dict\n",
    "def predict(img) -> Tuple[Dict, float]:\n",
    "  # Start a timer\n",
    "  start_time = timer()\n",
    "\n",
    "  # Transform the input image for use with EffNetB2\n",
    "  img = resnet50_transforms(img).unsqueeze(0) # unsqueeze = add batch dimension on 0th index\n",
    "\n",
    "  # Put model into eval mode, make prediction\n",
    "  resnet50_pets.eval()\n",
    "  with torch.inference_mode():\n",
    "    # Pass transformed image through the model and turn the prediction logits into probaiblities\n",
    "    pred_probs = torch.softmax(resnet50_pets(img), dim=1)\n",
    "\n",
    "  # Create a prediction label and prediction probability dictionary\n",
    "  # ความน่าจะเป็นการทำนาย (prediction probability)\n",
    "  pred_labels_and_probs = {class_names[i]: float(pred_probs[0][i]) for i in range(len(class_names))}\n",
    "\n",
    "  # Calculate pred time\n",
    "  end_time = timer()\n",
    "  pred_time = round(end_time - start_time, 4)\n",
    "\n",
    "  # Return pred dict and pred time\n",
    "  return pred_labels_and_probs, pred_time\n",
    "\n",
    "\n",
    "#create a example list\n",
    "example_list = [\"examples/\" + example for example in os.listdir(\"examples\")]\n",
    "example_list\n",
    "\n",
    "\n",
    "title = \"Resnet50 Pet 🐶🐱🐈\"\n",
    "description = \"An Resnet50 feature extractor computer vision model to classify Pet images into 37 classes Dog & Cat\"\n",
    "article = \" Created at [https://github.com/raksitkan/Pytorch_vision_Pet]).\"\n",
    "demo = gr.Interface(fn=predict,\n",
    "                    inputs=gr.Image(type=\"pil\"),\n",
    "                    outputs=[gr.Label(num_top_classes=3, label=\"Predictions\"),\n",
    "                             gr.Number(label=\"Prediction time (s)\")],\n",
    "                    examples=example_list,\n",
    "                    title=title,\n",
    "                    description=description,\n",
    "                    article=article)\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a requirements file for resnet50 pet (requirements.txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "print(f\"torch version: {torch.__version__}\")\n",
    "print(f\"torchvision version: {torchvision.__version__}\")\n",
    "print(f\"torchvision version: {gr.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile demos/requirements.txt\n",
    "torch==2.5.1\n",
    "torchvision==0.20.1\n",
    "gradio==5.7.1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zip file demos for upload to huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "shutil.make_archive('demos', 'zip', 'demos')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See our Porject below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IPython is a library to help make Python interactive\n",
    "from IPython.display import IFrame\n",
    "\n",
    "# Embed FoodVision Mini Gradio demo\n",
    "IFrame(src=\"https://raksitkan-resnet50-pet-classifier.hf.space\", width=900, height=750)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Deep_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
